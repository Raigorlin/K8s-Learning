# Bootstrapping the Kubernetes Worker Nodes

In this lab you will bootstrap three Kubernetes worker nodes. The following components will be installed on each node: [runc](https://github.com/opencontainers/runc), [container networking plugins](https://github.com/containernetworking/cni), [containerd](https://github.com/containerd/containerd), [kubelet](https://kubernetes.io/docs/admin/kubelet), and [kube-proxy](https://kubernetes.io/docs/concepts/cluster-administration/proxies).


## Prerequisites

The commands in this lab must be run on each worker instance: `k8s-worker1`, `k8s-worker2`, `k8s-worker3`.

### Enable Packet forwarding

```shell
vi /etc/sysctl.conf 
net.ipv4.ip_forward=1

sudo sysctl -p /etc/sysctl.conf
```

### Disable Swap

By default the kubelet will fail to start if [swap](https://help.ubuntu.com/community/SwapFaq) is enabled. It is [recommended](https://github.com/kubernetes/kubernetes/issues/7294) that swap be disabled to ensure Kubernetes can provide proper resource allocation and quality of service.

```shell
sudo swapon --show
``` 

Alternative way

```shell
sudo vim /etc/fstab
#/swap.img      none    swap    sw      0       0

sudo shutdown -r now
```

```shell
sudo swapoff -a
```

### Setup DNS

Normally we request DNS from local DNS record like host/resolv and for this tutorial we gonna skip `DNS stub resolver` and we will be requested from our `DHCP Server(Fortigate)` instead.

[<p align="center"><img src="./screenshots/dnsstubresolver.png" width="750"/></p>](dnsstubresolver.png)

```shell
sudo vim /etc/systemd/resolved.conf

# Uncomment this 
DNSStubListener=no

# Restart System DNS
sudo systemctl restart systemd-resolved
```


## Download and copy official install Worker binaries

```shell
wget -q \
  https://github.com/kubernetes-incubator/cri-tools/releases/download/v1.0.0-beta.0/crictl-v1.0.0-beta.0-linux-amd64.tar.gz \
  https://github.com/opencontainers/runc/releases/download/v1.0.0-rc5/runc.amd64 \
  https://github.com/containernetworking/plugins/releases/download/v0.6.0/cni-plugins-amd64-v0.6.0.tgz \
  https://github.com/containerd/containerd/releases/download/v1.1.0/containerd-1.1.0.linux-amd64.tar.gz \
  https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubectl \
  https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kube-proxy \
  https://storage.googleapis.com/kubernetes-release/release/v1.10.2/bin/linux/amd64/kubelet
```

Copy To worker nodes
```shell
for i in k8s-worker1 k8s-worker2 k8s-worker3; do
scp runc.amd64 kubectl kube-proxy kubelet \
crictl-v1.0.0-beta.0-linux-amd64.tar.gz cni-plugins-amd64-v0.6.0.tgz \
containerd-1.1.0.linux-amd64.tar.gz jacklin@${i}:~/
done
```

## Provisioning a Kubernetes Worker Node

```shell
{
  sudo yum install -y epel-release
  sudo yum install -y socat conntrack ipset
}
```

## Install Worker Binaries

Create the installation directories

```shell
sudo mkdir -p \
  /etc/cni/net.d \
  /opt/cni/bin \
  /var/lib/kubelet \
  /var/lib/kube-proxy \
  /var/lib/kubernetes \
  /var/run/kubernetes
```

Install the worker binaries

```shell
{
  mkdir containerd
  sudo tar -xvf crictl-v1.0.0-beta.0-linux-amd64.tar.gz -C /usr/local/bin/
  sudo tar -xvf containerd-1.1.0.linux-amd64.tar.gz -C containerd
  sudo tar -xvf cni-plugins-amd64-v0.6.0.tgz -C /opt/cni/bin/
  sudo mv runc.amd64 runc
  chmod +x kubectl kube-proxy kubelet runc 
  sudo mv kubectl kube-proxy kubelet runc /usr/local/bin/
  sudo mv containerd/bin/* /bin/
}
```

## Configure containerd (container runtime)

```
cat <<EOF | sudo tee /etc/systemd/system/containerd.service
[Unit]
Description=containerd container runtime
Documentation=https://containerd.io
After=network.target

[Service]
ExecStartPre=/sbin/modprobe overlay
ExecStart=/bin/containerd
Restart=always
RestartSec=5
Delegate=yes
KillMode=process
OOMScoreAdjust=-999
LimitNOFILE=1048576
LimitNPROC=infinity
LimitCORE=infinity

[Install]
WantedBy=multi-user.target
EOF
```

## Config the kubelet

```
{
  HOSTNAME=`hostname -s`
  sudo mv ${HOSTNAME}-key.pem ${HOSTNAME}.pem /var/lib/kubelet/
  sudo mv ${HOSTNAME}.kubeconfig /var/lib/kubelet/kubeconfig
  sudo mv ca.pem /var/lib/kubernetes/
}
```

Create the `kubelet-config.yaml` configuration file:

```
{
HOSTNAME=`hostname -s`

cat <<EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml
kind: KubeletConfiguration
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    enabled: true
  x509:
    clientCAFile: "/var/lib/kubernetes/ca.pem"
authorization:
  mode: Webhook
clusterDomain: "cluster.local"
clusterDNS:
  - "10.98.96.10"
podCIDR: "10.98.100.0/22"
runtimeRequestTimeout: "15m"
tlsCertFile: "/var/lib/kubelet/${HOSTNAME}.pem"
tlsPrivateKeyFile: "/var/lib/kubelet/${HOSTNAME}-key.pem"
EOF
}
```
> The `resolvConf` configuration is used to avoid loops when using CoreDNS for service discovery on systems running `systemd-resolved`.

Create the kubelet.service systemd unit file

`--allow-privileged=true` - fixed `Waiting` state (specified privileged container, but is disallowed) `--resolv-conf=/run/systemd/resolve/resolv.conf` - DNS Pod had `127.0.0.1` as a nameserver in `/etc/resolv.conf`, resolution was failing.

>if you don't add `--hostname-override=${HOSTNAME}` got an error like this
```
Unable to register node "XXX" with API server: nodes "XXX" is forbidden: node "XXX" cannot modify node "XXX"
```

```
{
HOSTNAME=`hostname -s`

cat <<EOF | sudo tee /etc/systemd/system/kubelet.service
[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/kubernetes/kubernetes
After=containerd.service
Requires=containerd.service

[Service]
ExecStart=/usr/local/bin/kubelet \\
  --hostname-override=${HOSTNAME} \\
  --config=/var/lib/kubelet/kubelet-config.yaml \\
  --container-runtime=remote \\
  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\
  --image-pull-progress-deadline=2m \\
  --kubeconfig=/var/lib/kubelet/kubeconfig \\
  --network-plugin=cni \\
  --register-node=true \\
  --allow-privileged=true \\
  --resolv-conf=/run/systemd/resolve/resolv.conf \\
  --v=2
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
}
```

## Configure the Kubernetes Proxy

```
sudo mv kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig
```

Create the `kube-proxy-config.yaml` configuration file:

```
cat <<EOF | sudo tee /var/lib/kube-proxy/kube-proxy-config.yaml
kind: KubeProxyConfiguration
apiVersion: kubeproxy.config.k8s.io/v1alpha1
clientConnection:
  kubeconfig: "/var/lib/kube-proxy/kubeconfig"
mode: "iptables"
clusterCIDR: "10.98.100.0/22"
EOF
```

Create the `kube-proxy.service` systemd unit file:

```
cat <<EOF | sudo tee /etc/systemd/system/kube-proxy.service
[Unit]
Description=Kubernetes Kube Proxy
Documentation=https://github.com/kubernetes/kubernetes

[Service]
ExecStart=/usr/local/bin/kube-proxy \\
  --config=/var/lib/kube-proxy/kube-proxy-config.yaml
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
EOF
```

## Start the Worker Services

```
{
  sudo systemctl daemon-reload
  sudo systemctl enable containerd kubelet kube-proxy
  sudo systemctl start containerd kubelet kube-proxy
}
```
> Remember to run the above commands on each worker node: k8s-worker1, k8s-worker2, and k8s-worker3.


## Configure CNI Networking

We do not need to configure cni as we will setup Weave and it will do the necessary setup.

if you have add below pod you will get the following `error`:
```
Container runtime network not ready: NetworkReady=false reason:NetworkPluginNotReady message:Network plugin returns error: cni plugin not initialized
```

After worker nodes registered we will use this command for testing
```
kubectl apply -f https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml
```

## Verification

```
kubectl get nodes --kubeconfig admin.kubeconfig
```

```
kubectl get pods --all-namespaces
```
Result: 

```
NAMESPACE     NAME              READY   STATUS    RESTARTS   AGE
kube-system   weave-net-622g7   2/2     Running   1          11m
kube-system   weave-net-x4x2l   2/2     Running   1          11m
kube-system   weave-net-zwtxl   2/2     Running   1          11m
```